{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee802ed-e405-4f86-985a-c6e0656c5265",
   "metadata": {},
   "source": [
    "# Part IV: Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8dffd0-604c-4434-9f56-9f67578d60a2",
   "metadata": {},
   "source": [
    "## Basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a20043-c34e-41ec-a61f-d10c46238fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bae6ca2-7d52-4ed1-90e0-bf5dcb510dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fefc9dd-198a-4604-9ba3-c96566203eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = os.path.abspath(os.path.join(\"../..\", \"src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f3a359-c913-4b1b-8329-3a7a3c8c982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plots import plot_forecast_single\n",
    "from utils.utils import flatten_prophet_predictions, weighted_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10499f54-a5e2-47e2-9460-daaeca91675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature data: (686187, 89)\n",
      "Kaggle test rows: 526917\n",
      "Train rows: 159270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = \"../../data\"\n",
    "\n",
    "# 1. LOAD DATA ĐÃ PREPROCESS VÀ FEATURE ENGINEERING\n",
    "df_sales = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"data_processed/sales_data_preprocessed.csv\"),\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "df_weather = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"data_processed/weather_preprocessed.csv\"),\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "df_weather_key_store_merged = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"data_processed/weather_key_store_merged.csv\"),\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "\n",
    "# Đây là file đã có is_kaggle_test và toàn bộ features\n",
    "df_features = pd.read_feather(os.path.join(DATA_DIR,'data_processed/feature_engineered_data_89_features.feather'))\n",
    "\n",
    "print(\"Full feature data:\", df_features.shape)\n",
    "print(\"Kaggle test rows:\", df_features['is_kaggle_test'].sum())\n",
    "print(\"Train rows:\", (df_features['is_kaggle_test'] == 0).sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a351e769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'store_nbr', 'item_nbr', 'units', 'logunits', 'is_kaggle_test',\n",
       "       'station_nbr', 'tmax', 'tmin', 'tavg', 'depart', 'dewpoint', 'wetbulb',\n",
       "       'heat', 'cool', 'sunrise', 'sunset', 'snowfall', 'preciptotal',\n",
       "       'stnpressure', 'sealevel', 'resultspeed', 'resultdir', 'avgspeed',\n",
       "       'BCFG', 'BLDU', 'BLSN', 'BR', 'DU', 'DZ', 'FG', 'FG+', 'FU', 'FZDZ',\n",
       "       'FZFG', 'FZRA', 'GR', 'GS', 'HZ', 'MIFG', 'PL', 'PRFG', 'RA', 'SG',\n",
       "       'SN', 'SQ', 'TS', 'TSRA', 'TSSN', 'UP', 'VCFG', 'VCTS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather_key_store_merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12747917-837a-4b8a-9fc2-502deb096ea5",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f6afa0d-32fe-4192-b89c-00b7d9cf11e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final splits:\n",
      "  Train: (153496, 90)\n",
      "  Valid: (5774, 90)\n",
      "  Kaggle test: (526917, 90)\n"
     ]
    }
   ],
   "source": [
    "df_features['is_valid'] = 0\n",
    "mask_train = df_features['is_kaggle_test'] == 0\n",
    "cutoff_date = pd.Timestamp(\"2014-08-01\")\n",
    "df_features.loc[mask_train & (df_features['date'] >= cutoff_date), 'is_valid'] = 1\n",
    "\n",
    "# 2. Tách train/valid và kaggle test\n",
    "df_train = df_features[(df_features['is_kaggle_test'] == 0) & (df_features['is_valid'] == 0)].copy()\n",
    "df_valid = df_features[(df_features['is_kaggle_test'] == 0) & (df_features['is_valid'] == 1)].copy()\n",
    "df_kaggle_test = df_features[df_features['is_kaggle_test'] == 1].copy()\n",
    "\n",
    "print(\"Final splits:\")\n",
    "print(\"  Train:\", df_train.shape)\n",
    "print(\"  Valid:\", df_valid.shape)\n",
    "print(\"  Kaggle test:\", df_kaggle_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b12af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>units</th>\n",
       "      <th>logunits</th>\n",
       "      <th>is_kaggle_test</th>\n",
       "      <th>station_nbr</th>\n",
       "      <th>tmax</th>\n",
       "      <th>depart</th>\n",
       "      <th>cool</th>\n",
       "      <th>...</th>\n",
       "      <th>logunits_ewma_14d_a05</th>\n",
       "      <th>logunits_ewma_28d_a05</th>\n",
       "      <th>logunits_ewma_7d_a075</th>\n",
       "      <th>logunits_ewma_14d_a075</th>\n",
       "      <th>logunits_ewma_28d_a075</th>\n",
       "      <th>store_sum_7d</th>\n",
       "      <th>store_mean_7d</th>\n",
       "      <th>item_sum_7d</th>\n",
       "      <th>item_mean_7d</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159270</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.670772</td>\n",
       "      <td>1.238682</td>\n",
       "      <td>7.203406</td>\n",
       "      <td>1.029058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159271</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.825560</td>\n",
       "      <td>1.260794</td>\n",
       "      <td>6.510258</td>\n",
       "      <td>1.085043</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159272</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.102488</td>\n",
       "      <td>1.728927</td>\n",
       "      <td>5.817111</td>\n",
       "      <td>1.163422</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159273</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>86.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.648221</td>\n",
       "      <td>1.235460</td>\n",
       "      <td>5.123964</td>\n",
       "      <td>1.280991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159274</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.572480</td>\n",
       "      <td>1.367497</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>1.059351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  store_nbr  item_nbr  units  logunits  is_kaggle_test  \\\n",
       "159270 2013-04-01          2         1    NaN       NaN            True   \n",
       "159271 2013-04-01          3         1    NaN       NaN            True   \n",
       "159272 2013-04-01          6         1    NaN       NaN            True   \n",
       "159273 2013-04-01          7         1    NaN       NaN            True   \n",
       "159274 2013-04-01          8         1    NaN       NaN            True   \n",
       "\n",
       "        station_nbr  tmax  depart  cool  ...  logunits_ewma_14d_a05  \\\n",
       "159270           14  71.0     1.0   0.0  ...                    NaN   \n",
       "159271            7  68.0     6.2   0.0  ...                    NaN   \n",
       "159272           14  71.0     1.0   0.0  ...                    NaN   \n",
       "159273            6  86.0     6.0   5.0  ...                    NaN   \n",
       "159274            4  87.0     8.0   9.0  ...                    NaN   \n",
       "\n",
       "        logunits_ewma_28d_a05  logunits_ewma_7d_a075  logunits_ewma_14d_a075  \\\n",
       "159270                    NaN                    NaN                     NaN   \n",
       "159271                    NaN                    NaN                     NaN   \n",
       "159272                    NaN                    NaN                     NaN   \n",
       "159273                    NaN                    NaN                     NaN   \n",
       "159274                    NaN                    NaN                     NaN   \n",
       "\n",
       "        logunits_ewma_28d_a075  store_sum_7d  store_mean_7d  item_sum_7d  \\\n",
       "159270                     NaN      8.670772       1.238682     7.203406   \n",
       "159271                     NaN      8.825560       1.260794     6.510258   \n",
       "159272                     NaN     12.102488       1.728927     5.817111   \n",
       "159273                     NaN      8.648221       1.235460     5.123964   \n",
       "159274                     NaN      9.572480       1.367497     3.178054   \n",
       "\n",
       "        item_mean_7d  is_valid  \n",
       "159270      1.029058         0  \n",
       "159271      1.085043         0  \n",
       "159272      1.163422         0  \n",
       "159273      1.280991         0  \n",
       "159274      1.059351         0  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kaggle_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e5c03b-dfdb-4bdf-81e9-b811b98e50d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 153496 entries, 0 to 159249\n",
      "Data columns (total 90 columns):\n",
      " #   Column                  Non-Null Count   Dtype         \n",
      "---  ------                  --------------   -----         \n",
      " 0   date                    153496 non-null  datetime64[ns]\n",
      " 1   store_nbr               153496 non-null  int64         \n",
      " 2   item_nbr                153496 non-null  int64         \n",
      " 3   units                   153496 non-null  float64       \n",
      " 4   logunits                153496 non-null  float64       \n",
      " 5   is_kaggle_test          153496 non-null  bool          \n",
      " 6   station_nbr             153496 non-null  int64         \n",
      " 7   tmax                    153496 non-null  float64       \n",
      " 8   depart                  153496 non-null  float64       \n",
      " 9   cool                    153496 non-null  float64       \n",
      " 10  sunrise                 153496 non-null  float64       \n",
      " 11  sunset                  153496 non-null  float64       \n",
      " 12  snowfall                153496 non-null  float64       \n",
      " 13  preciptotal             153496 non-null  float64       \n",
      " 14  stnpressure             153496 non-null  float64       \n",
      " 15  sealevel                153496 non-null  float64       \n",
      " 16  resultspeed             153496 non-null  float64       \n",
      " 17  resultdir               153496 non-null  float64       \n",
      " 18  BCFG                    153496 non-null  int64         \n",
      " 19  BLDU                    153496 non-null  int64         \n",
      " 20  BLSN                    153496 non-null  int64         \n",
      " 21  BR                      153496 non-null  int64         \n",
      " 22  DU                      153496 non-null  int64         \n",
      " 23  DZ                      153496 non-null  int64         \n",
      " 24  FG                      153496 non-null  int64         \n",
      " 25  FG+                     153496 non-null  int64         \n",
      " 26  FU                      153496 non-null  int64         \n",
      " 27  FZDZ                    153496 non-null  int64         \n",
      " 28  FZFG                    153496 non-null  int64         \n",
      " 29  FZRA                    153496 non-null  int64         \n",
      " 30  GR                      153496 non-null  int64         \n",
      " 31  GS                      153496 non-null  int64         \n",
      " 32  HZ                      153496 non-null  int64         \n",
      " 33  MIFG                    153496 non-null  int64         \n",
      " 34  PL                      153496 non-null  int64         \n",
      " 35  PRFG                    153496 non-null  int64         \n",
      " 36  RA                      153496 non-null  int64         \n",
      " 37  SG                      153496 non-null  int64         \n",
      " 38  SN                      153496 non-null  int64         \n",
      " 39  SQ                      153496 non-null  int64         \n",
      " 40  TS                      153496 non-null  int64         \n",
      " 41  TSRA                    153496 non-null  int64         \n",
      " 42  TSSN                    153496 non-null  int64         \n",
      " 43  UP                      153496 non-null  int64         \n",
      " 44  VCFG                    153496 non-null  int64         \n",
      " 45  VCTS                    153496 non-null  int64         \n",
      " 46  year                    153496 non-null  int32         \n",
      " 47  month                   153496 non-null  int32         \n",
      " 48  day                     153496 non-null  int32         \n",
      " 49  day_of_week             153496 non-null  int32         \n",
      " 50  is_weekend              153496 non-null  int64         \n",
      " 51  season                  153496 non-null  int64         \n",
      " 52  season_Spring           153496 non-null  int64         \n",
      " 53  season_Summer           153496 non-null  int64         \n",
      " 54  season_Winter           153496 non-null  int64         \n",
      " 55  is_holiday              153496 non-null  int64         \n",
      " 56  is_blackfriday          153496 non-null  int64         \n",
      " 57  logunits_lag_1          153496 non-null  float64       \n",
      " 58  logunits_lag_2          153496 non-null  float64       \n",
      " 59  logunits_lag_3          153496 non-null  float64       \n",
      " 60  logunits_lag_4          153496 non-null  float64       \n",
      " 61  logunits_lag_5          153496 non-null  float64       \n",
      " 62  logunits_lag_6          153496 non-null  float64       \n",
      " 63  logunits_lag_7          153496 non-null  float64       \n",
      " 64  logunits_lag_14         153496 non-null  float64       \n",
      " 65  logunits_lag_21         153496 non-null  float64       \n",
      " 66  logunits_lag_28         153496 non-null  float64       \n",
      " 67  logunits_mean_7d        153496 non-null  float64       \n",
      " 68  logunits_min_7d         153496 non-null  float64       \n",
      " 69  logunits_max_7d         153496 non-null  float64       \n",
      " 70  logunits_std_7d         153496 non-null  float64       \n",
      " 71  logunits_mean_14d       153496 non-null  float64       \n",
      " 72  logunits_min_14d        153496 non-null  float64       \n",
      " 73  logunits_max_14d        153496 non-null  float64       \n",
      " 74  logunits_std_14d        153496 non-null  float64       \n",
      " 75  logunits_mean_28d       153496 non-null  float64       \n",
      " 76  logunits_min_28d        153496 non-null  float64       \n",
      " 77  logunits_max_28d        153496 non-null  float64       \n",
      " 78  logunits_std_28d        153496 non-null  float64       \n",
      " 79  logunits_ewma_7d_a05    153496 non-null  float64       \n",
      " 80  logunits_ewma_14d_a05   153496 non-null  float64       \n",
      " 81  logunits_ewma_28d_a05   153496 non-null  float64       \n",
      " 82  logunits_ewma_7d_a075   153496 non-null  float64       \n",
      " 83  logunits_ewma_14d_a075  153496 non-null  float64       \n",
      " 84  logunits_ewma_28d_a075  153496 non-null  float64       \n",
      " 85  store_sum_7d            153496 non-null  float64       \n",
      " 86  store_mean_7d           153496 non-null  float64       \n",
      " 87  item_sum_7d             153496 non-null  float64       \n",
      " 88  item_mean_7d            153496 non-null  float64       \n",
      " 89  is_valid                153496 non-null  int64         \n",
      "dtypes: bool(1), datetime64[ns](1), float64(45), int32(4), int64(39)\n",
      "memory usage: 103.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630715f-21ce-4859-8325-6f3c8956058b",
   "metadata": {},
   "source": [
    "## Build xboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2063b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping data for XGBoost...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape before: (153496, 90) -> after: (153496, 90)\n",
      "Valid shape before: (5774, 90) -> after: (5774, 90)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [c for c in df_train.columns if c not in ['date', 'store_nbr', 'item_nbr', 'units', 'logunits']]\n",
    "\n",
    "# Định nghĩa dictionary aggregation\n",
    "agg_dict = {\n",
    "    'logunits': 'sum',  # Target chính\n",
    "    'units': 'sum',     # Target phụ (để tham chiếu)\n",
    "}\n",
    "\n",
    "# Với tất cả feature khác, lấy 'first' (giả định dữ liệu duplicate đã giống nhau về feature)\n",
    "for c in feature_cols:\n",
    "    agg_dict[c] = 'first' \n",
    "\n",
    "# Groupby & Aggregation\n",
    "print(\"Grouping data for XGBoost...\")\n",
    "df_train_grouped = df_train.groupby(['store_nbr', 'item_nbr', 'date'], as_index=False).agg(agg_dict)\n",
    "df_valid_grouped = df_valid.groupby(['store_nbr', 'item_nbr', 'date'], as_index=False).agg(agg_dict)\n",
    "\n",
    "print(f\"Train shape before: {df_train.shape} -> after: {df_train_grouped.shape}\")\n",
    "print(f\"Valid shape before: {df_valid.shape} -> after: {df_valid_grouped.shape}\")\n",
    "\n",
    "drop_train_cols = ['date', 'units', 'logunits', 'store_nbr', 'item_nbr', \n",
    "                   'station_nbr', 'is_kaggle_test', 'is_valid']\n",
    "\n",
    "# Tách lại X, y từ dữ liệu đã grouped\n",
    "X_train = df_train_grouped.drop(columns=drop_train_cols) \n",
    "y_train = df_train_grouped['logunits']\n",
    "\n",
    "X_valid = df_valid_grouped.drop(columns=drop_train_cols)\n",
    "y_valid = df_valid_grouped['logunits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3bdc1-c273-4ee0-8f7f-923897e4cf84",
   "metadata": {},
   "source": [
    "### Build a lightgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea66f23-83e6-45d6-90d0-dd262986d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xgboost_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Create an XGBoost model using engineered features\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating base XGBoost model...\")\n",
    "    \n",
    "    \n",
    "    # Use a time series split for validation within the training set\n",
    "    # This ensures we're always validating on future data\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Basic XGBoost parameters (equivalent to LightGBM params)\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"max_depth\": 6,  # Equivalent to num_leaves=31\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"subsample\": 0.9,  # Equivalent to feature_fraction\n",
    "        \"n_estimators\": 100,\n",
    "        \"verbosity\": 0,  # Equivalent to verbose=-1\n",
    "    }\n",
    "    \n",
    "    # Train the model with cross-validation on training data only\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_train_cv, X_val_cv = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_cv, y_val_cv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Prepare DMatrix for XGBoost (more efficient)\n",
    "        dtrain = xgb.DMatrix(X_train_cv, label=y_train_cv)\n",
    "        dval = xgb.DMatrix(X_val_cv, label=y_val_cv)\n",
    "        \n",
    "        # Train the model\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=100,\n",
    "            evals=[(dval, \"validation\")],\n",
    "            # early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        dval_pred = xgb.DMatrix(X_val_cv)\n",
    "        preds = model.predict(dval_pred)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_val_cv, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_cv, preds))\n",
    "        wape = weighted_absolute_percentage_error(y_val_cv, preds)\n",
    "        \n",
    "        cv_scores.append((mae, rmse, wape))\n",
    "    \n",
    "    # Print average scores from cross-validation\n",
    "    mae_avg, rmse_avg, wape_avg = np.mean(cv_scores, axis=0)\n",
    "    print(\n",
    "        f\"Baseline Model CV - MAE: {mae_avg:.2f}, RMSE: {rmse_avg:.2f}, WAPE: {wape_avg:.2f}%\"\n",
    "    )\n",
    "    \n",
    "    # Train a final model on all training data\n",
    "    dtrain_final = xgb.DMatrix(X_train, label=y_train)\n",
    "    final_model = xgb.train(params, dtrain_final, num_boost_round=100, verbose_eval=False)\n",
    "    \n",
    "    # Evaluate on the test set (last 3 months of 2017)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    test_preds = final_model.predict(dtest)\n",
    "    test_mae = mean_absolute_error(y_test, test_preds)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "    test_wape = weighted_absolute_percentage_error(y_test, test_preds)\n",
    "    \n",
    "    print(\n",
    "        f\"Baseline Model Test - MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, WAPE: {test_wape:.2f}%\"\n",
    "    )\n",
    "    \n",
    "    return final_model, (test_mae, test_rmse, test_wape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f52819f7-c5fa-43b4-99b5-01dc13a5e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating base XGBoost model...\n",
      "Baseline Model CV - MAE: 0.30, RMSE: 0.46, WAPE: 19.11%\n",
      "Baseline Model Test - MAE: 0.21, RMSE: 0.36, WAPE: 15.14%\n"
     ]
    }
   ],
   "source": [
    "# Gọi hàm train\n",
    "model, metrics = create_xgboost_model(\n",
    "    X_train, y_train, X_valid, y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beff77fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Model Results:\n",
      "MAE: 0.21 | RMSE: 0.36 | WAPE: 15.14%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of Prophet Model\n",
    "print(\n",
    "    f\"LightGBM Model Results:\\nMAE: {metrics[0]:.2f} | RMSE: {metrics[1]:.2f} | WAPE: {metrics[2]:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1382e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['is_kaggle_test', 'station_nbr', 'tmax', 'depart', 'cool', 'sunrise', 'sunset', 'snowfall', 'preciptotal', 'stnpressure', 'sealevel', 'resultspeed', 'resultdir', 'BCFG', 'BLDU', 'BLSN', 'BR', 'DU', 'DZ', 'FG', 'FG+', 'FU', 'FZDZ', 'FZFG', 'FZRA', 'GR', 'GS', 'HZ', 'MIFG', 'PL', 'PRFG', 'RA', 'SG', 'SN', 'SQ', 'TS', 'TSRA', 'TSSN', 'UP', 'VCFG', 'VCTS', 'year', 'month', 'day', 'day_of_week', 'is_weekend', 'season', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday', 'is_blackfriday', 'logunits_lag_1', 'logunits_lag_2', 'logunits_lag_3', 'logunits_lag_4', 'logunits_lag_5', 'logunits_lag_6', 'logunits_lag_7', 'logunits_lag_14', 'logunits_lag_21', 'logunits_lag_28', 'logunits_mean_7d', 'logunits_min_7d', 'logunits_max_7d', 'logunits_std_7d', 'logunits_mean_14d', 'logunits_min_14d', 'logunits_max_14d', 'logunits_std_14d', 'logunits_mean_28d', 'logunits_min_28d', 'logunits_max_28d', 'logunits_std_28d', 'logunits_ewma_7d_a05', 'logunits_ewma_14d_a05', 'logunits_ewma_28d_a05', 'logunits_ewma_7d_a075', 'logunits_ewma_14d_a075', 'logunits_ewma_28d_a075', 'store_sum_7d', 'store_mean_7d', 'item_sum_7d', 'item_mean_7d', 'is_valid'] ['tmax', 'depart', 'cool', 'sunrise', 'sunset', 'snowfall', 'preciptotal', 'stnpressure', 'sealevel', 'resultspeed', 'resultdir', 'BCFG', 'BLDU', 'BLSN', 'BR', 'DU', 'DZ', 'FG', 'FG+', 'FU', 'FZDZ', 'FZFG', 'FZRA', 'GR', 'GS', 'HZ', 'MIFG', 'PL', 'PRFG', 'RA', 'SG', 'SN', 'SQ', 'TS', 'TSRA', 'TSSN', 'UP', 'VCFG', 'VCTS', 'year', 'month', 'day', 'day_of_week', 'is_weekend', 'season', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday', 'is_blackfriday', 'logunits_lag_1', 'logunits_lag_2', 'logunits_lag_3', 'logunits_lag_4', 'logunits_lag_5', 'logunits_lag_6', 'logunits_lag_7', 'logunits_lag_14', 'logunits_lag_21', 'logunits_lag_28', 'logunits_mean_7d', 'logunits_min_7d', 'logunits_max_7d', 'logunits_std_7d', 'logunits_mean_14d', 'logunits_min_14d', 'logunits_max_14d', 'logunits_std_14d', 'logunits_mean_28d', 'logunits_min_28d', 'logunits_max_28d', 'logunits_std_28d', 'logunits_ewma_7d_a05', 'logunits_ewma_14d_a05', 'logunits_ewma_28d_a05', 'logunits_ewma_7d_a075', 'logunits_ewma_14d_a075', 'logunits_ewma_28d_a075', 'store_sum_7d', 'store_mean_7d', 'item_sum_7d', 'item_mean_7d']\nexpected station_nbr, is_kaggle_test, is_valid in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(submission)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m submission\n\u001b[0;32m---> 50\u001b[0m submission \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_xgboost_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_kaggle_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m, in \u001b[0;36mcreate_xgboost_submission\u001b[0;34m(df_kaggle_test, lightgbm_model, filename)\u001b[0m\n\u001b[1;32m     25\u001b[0m dtest \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_kaggle)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 3. Predict (trên log scale) và inverse transform\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[43mlightgbm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m df_kaggle_pred \u001b[38;5;241m=\u001b[39m df_ids\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     30\u001b[0m df_kaggle_pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myhat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m yhat\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/xgboost/core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/xgboost/core.py:2689\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, output_margin, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\u001b[0m\n\u001b[1;32m   2687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[1;32m   2688\u001b[0m     fn \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfeature_names\n\u001b[0;32m-> 2689\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2690\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2691\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2692\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m: training,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2695\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m: strict_shape,\n\u001b[1;32m   2696\u001b[0m }\n\u001b[1;32m   2698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21massign_type\u001b[39m(t: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/xgboost/core.py:3431\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[0;34m(self, feature_names)\u001b[0m\n\u001b[1;32m   3425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m my_missing:\n\u001b[1;32m   3426\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtraining data did not have the following fields: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3428\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m my_missing)\n\u001b[1;32m   3429\u001b[0m     )\n\u001b[0;32m-> 3431\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['is_kaggle_test', 'station_nbr', 'tmax', 'depart', 'cool', 'sunrise', 'sunset', 'snowfall', 'preciptotal', 'stnpressure', 'sealevel', 'resultspeed', 'resultdir', 'BCFG', 'BLDU', 'BLSN', 'BR', 'DU', 'DZ', 'FG', 'FG+', 'FU', 'FZDZ', 'FZFG', 'FZRA', 'GR', 'GS', 'HZ', 'MIFG', 'PL', 'PRFG', 'RA', 'SG', 'SN', 'SQ', 'TS', 'TSRA', 'TSSN', 'UP', 'VCFG', 'VCTS', 'year', 'month', 'day', 'day_of_week', 'is_weekend', 'season', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday', 'is_blackfriday', 'logunits_lag_1', 'logunits_lag_2', 'logunits_lag_3', 'logunits_lag_4', 'logunits_lag_5', 'logunits_lag_6', 'logunits_lag_7', 'logunits_lag_14', 'logunits_lag_21', 'logunits_lag_28', 'logunits_mean_7d', 'logunits_min_7d', 'logunits_max_7d', 'logunits_std_7d', 'logunits_mean_14d', 'logunits_min_14d', 'logunits_max_14d', 'logunits_std_14d', 'logunits_mean_28d', 'logunits_min_28d', 'logunits_max_28d', 'logunits_std_28d', 'logunits_ewma_7d_a05', 'logunits_ewma_14d_a05', 'logunits_ewma_28d_a05', 'logunits_ewma_7d_a075', 'logunits_ewma_14d_a075', 'logunits_ewma_28d_a075', 'store_sum_7d', 'store_mean_7d', 'item_sum_7d', 'item_mean_7d', 'is_valid'] ['tmax', 'depart', 'cool', 'sunrise', 'sunset', 'snowfall', 'preciptotal', 'stnpressure', 'sealevel', 'resultspeed', 'resultdir', 'BCFG', 'BLDU', 'BLSN', 'BR', 'DU', 'DZ', 'FG', 'FG+', 'FU', 'FZDZ', 'FZFG', 'FZRA', 'GR', 'GS', 'HZ', 'MIFG', 'PL', 'PRFG', 'RA', 'SG', 'SN', 'SQ', 'TS', 'TSRA', 'TSSN', 'UP', 'VCFG', 'VCTS', 'year', 'month', 'day', 'day_of_week', 'is_weekend', 'season', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday', 'is_blackfriday', 'logunits_lag_1', 'logunits_lag_2', 'logunits_lag_3', 'logunits_lag_4', 'logunits_lag_5', 'logunits_lag_6', 'logunits_lag_7', 'logunits_lag_14', 'logunits_lag_21', 'logunits_lag_28', 'logunits_mean_7d', 'logunits_min_7d', 'logunits_max_7d', 'logunits_std_7d', 'logunits_mean_14d', 'logunits_min_14d', 'logunits_max_14d', 'logunits_std_14d', 'logunits_mean_28d', 'logunits_min_28d', 'logunits_max_28d', 'logunits_std_28d', 'logunits_ewma_7d_a05', 'logunits_ewma_14d_a05', 'logunits_ewma_28d_a05', 'logunits_ewma_7d_a075', 'logunits_ewma_14d_a075', 'logunits_ewma_28d_a075', 'store_sum_7d', 'store_mean_7d', 'item_sum_7d', 'item_mean_7d']\nexpected station_nbr, is_kaggle_test, is_valid in input data"
     ]
    }
   ],
   "source": [
    "def create_xgboost_submission(df_kaggle_test, lightgbm_model, filename=\"submission_xgboost.csv\"):\n",
    "    \"\"\"\n",
    "    Tạo file submission từ model LightGBM đã train.\n",
    "    - df_kaggle_test: full test dataframe (có cột is_kaggle_test, date, store_nbr, item_nbr, ...).\n",
    "    - lightgbm_model: model đã fit trên logunits.\n",
    "    - filename: tên file csv output.\n",
    "    \"\"\"\n",
    "    # 1. Lọc đúng dữ liệu cho tập Test (từ 01/04/2013 trở đi)\n",
    "    min_test_date = \"2013-04-01\"\n",
    "    df_kaggle_test_lgbm = df_kaggle_test[\n",
    "        (df_kaggle_test['is_kaggle_test'] == True) &\n",
    "        (df_kaggle_test['date'] >= min_test_date)\n",
    "    ].copy()\n",
    "\n",
    "    # 2. Tách ID columns + Features (không dùng store_nbr, item_nbr cho model)\n",
    "    id_cols = ['store_nbr', 'item_nbr', 'date']\n",
    "    drop_cols = [\n",
    "        'date', 'units', 'logunits',\n",
    "        'is_kaggle_test', 'is_valid', 'station_nbr',\n",
    "        'store_nbr', 'item_nbr'\n",
    "    ]\n",
    "\n",
    "    df_ids = df_kaggle_test_lgbm[id_cols].copy()\n",
    "    X_kaggle = df_kaggle_test_lgbm.drop(columns=drop_cols)\n",
    "    dtest = xgb.DMatrix(X_kaggle)\n",
    "\n",
    "    # 3. Predict (trên log scale) và inverse transform\n",
    "    yhat = lightgbm_model.predict(dtest)\n",
    "    df_kaggle_pred = df_ids.copy()\n",
    "    df_kaggle_pred['yhat'] = yhat\n",
    "    df_kaggle_pred['units'] = np.expm1(df_kaggle_pred['yhat']).clip(lower=0)\n",
    "\n",
    "    # 4. Tạo date_str, sort và ID đúng format Kaggle\n",
    "    df_kaggle_pred['date_str'] = df_kaggle_pred['date'].dt.strftime('%Y-%m-%d')\n",
    "    df_kaggle_pred = df_kaggle_pred.sort_values(['date_str', 'store_nbr', 'item_nbr'])\n",
    "\n",
    "    df_kaggle_pred['id'] = (\n",
    "        df_kaggle_pred['store_nbr'].astype(str) + '_' +\n",
    "        df_kaggle_pred['item_nbr'].astype(str) + '_' +\n",
    "        df_kaggle_pred['date_str']\n",
    "    )\n",
    "\n",
    "    # 5. Tạo submission và lưu\n",
    "    submission = df_kaggle_pred[['id', 'units']].reset_index(drop=True)\n",
    "    submission.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"✓ Saved {filename} ({len(submission)} rows)\")\n",
    "    return submission\n",
    "\n",
    "submission = create_xgboost_submission(df_kaggle_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc79b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng dòng có dự đoán bán hàng: 526778\n",
      "Ví dụ 5 dòng có số liệu:\n",
      "               id       units\n",
      "0  2_1_2013-04-01  263.838989\n",
      "1  2_2_2013-04-01  255.323090\n",
      "2  2_3_2013-04-01  255.323090\n",
      "3  2_4_2013-04-01  247.580109\n",
      "4  2_5_2013-04-01   61.911465\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra nhanh: Đếm số lượng dòng dự đoán khác 0\n",
    "non_zero_preds = submission[submission['units'] > 0]\n",
    "print(f\"Số lượng dòng có dự đoán bán hàng: {len(non_zero_preds)}\")\n",
    "print(\"Ví dụ 5 dòng có số liệu:\")\n",
    "print(non_zero_preds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8bad5-ddbd-4091-ba03-5bcfbe412736",
   "metadata": {},
   "source": [
    "### (Optional) Fine tunning using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91197807-9c05-43ad-bcd0-4040f913a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(X_train, y_train, X_valid, y_valid, n_trials=50):\n",
    "    print(\"\\nOptimizing XGBoost model with Optuna...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Hyperparameters search space for XGBoost\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"verbosity\": 0,\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "            \"n_estimators\": 2000,\n",
    "        }\n",
    "        \n",
    "        # Prepare DMatrix\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        evals_result = {}\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=2000,\n",
    "            evals=[(dvalid, \"validation\")],\n",
    "            evals_result=evals_result,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Predict và tính metric mục tiêu (WAPE)\n",
    "        dvalid_pred = xgb.DMatrix(X_valid)\n",
    "        preds = model.predict(dvalid_pred)\n",
    "        # wape = weighted_absolute_percentage_error(y_valid, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "        return rmse  # Optimize trực tiếp WAPE\n",
    "    \n",
    "    # Chạy Optuna\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(\"\\nBest params found:\")\n",
    "    best_params = study.best_params\n",
    "    best_params.update({\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"verbosity\": 0,\n",
    "        \"n_estimators\": 2000\n",
    "    })\n",
    "    \n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Train final model với best params\n",
    "    dtrain_final = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid_final = xgb.DMatrix(X_valid, label=y_valid)\n",
    "    \n",
    "    evals_result_final = {}\n",
    "    final_model = xgb.train(\n",
    "        best_params,\n",
    "        dtrain_final,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dvalid_final, \"validation\")],\n",
    "        evals_result=evals_result_final,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "\n",
    "    dvalid_pred = xgb.DMatrix(X_valid)\n",
    "    valid_preds = final_model.predict(dvalid_pred)\n",
    "    test_mae = mean_absolute_error(y_valid, valid_preds)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_valid, valid_preds))\n",
    "    test_wape = weighted_absolute_percentage_error(y_valid, valid_preds)\n",
    "    \n",
    "    print(f\"\\nOptimized XGBoost Valid Metrics - MAE: {test_mae:.3f}, RMSE: {test_rmse:.3f}, WAPE: {test_wape:.3f}\")\n",
    "    \n",
    "    return final_model, best_params, (test_mae, test_rmse, test_wape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ebd4d-bf01-4e6a-9976-e9574eb7767c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 03:57:26,003] A new study created in memory with name: no-name-90da8ea3-ce37-4bfb-961c-366e9757a584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing XGBoost model with Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 03:57:34,332] Trial 0 finished with value: 0.365722180603747 and parameters: {'max_depth': 3, 'learning_rate': 0.01903534445090596, 'subsample': 0.6902107493332166, 'colsample_bytree': 0.9004935228542755, 'colsample_bylevel': 0.8195214171939714, 'min_child_weight': 5, 'gamma': 3.076832793306713, 'reg_alpha': 3.151856034280738e-05, 'reg_lambda': 1.6538936656900057e-08}. Best is trial 0 with value: 0.365722180603747.\n",
      "[I 2025-11-29 03:57:35,368] Trial 1 finished with value: 0.366993785285186 and parameters: {'max_depth': 8, 'learning_rate': 0.25058579795192265, 'subsample': 0.9838603251295657, 'colsample_bytree': 0.9705700444081873, 'colsample_bylevel': 0.5516388480799873, 'min_child_weight': 4, 'gamma': 1.5216596832049556, 'reg_alpha': 4.7570761405595006e-07, 'reg_lambda': 0.0003499853677671967}. Best is trial 0 with value: 0.365722180603747.\n",
      "[I 2025-11-29 03:57:40,988] Trial 2 finished with value: 0.3637522390482898 and parameters: {'max_depth': 4, 'learning_rate': 0.029649144334357953, 'subsample': 0.7818180907857128, 'colsample_bytree': 0.7688507111390985, 'colsample_bylevel': 0.7145353521506144, 'min_child_weight': 2, 'gamma': 0.5799294960679818, 'reg_alpha': 0.06083821642128527, 'reg_lambda': 3.746288275742337e-06}. Best is trial 2 with value: 0.3637522390482898.\n",
      "[I 2025-11-29 03:57:48,875] Trial 3 finished with value: 0.36299408724650967 and parameters: {'max_depth': 5, 'learning_rate': 0.01854317741806545, 'subsample': 0.9244135174765666, 'colsample_bytree': 0.5870166854668846, 'colsample_bylevel': 0.938273894532897, 'min_child_weight': 5, 'gamma': 2.031306884026037, 'reg_alpha': 5.151808977252315, 'reg_lambda': 0.00024184369194330015}. Best is trial 3 with value: 0.36299408724650967.\n",
      "[I 2025-11-29 03:57:52,949] Trial 4 finished with value: 0.3637491759894264 and parameters: {'max_depth': 10, 'learning_rate': 0.06778828214804584, 'subsample': 0.748814905666766, 'colsample_bytree': 0.60060934099737, 'colsample_bylevel': 0.884144543276652, 'min_child_weight': 8, 'gamma': 3.210602696388683, 'reg_alpha': 9.621641263219188e-06, 'reg_lambda': 0.0026597737346840122}. Best is trial 3 with value: 0.36299408724650967.\n",
      "[W 2025-11-29 03:57:58,205] Trial 5 failed with parameters: {'max_depth': 5, 'learning_rate': 0.04691202768406318, 'subsample': 0.8403242641640347, 'colsample_bytree': 0.84563809201904, 'colsample_bylevel': 0.7096928734249583, 'min_child_weight': 8, 'gamma': 3.622659609749996, 'reg_alpha': 0.0009298773003537166, 'reg_lambda': 0.08984255983429362} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/research/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_296158/1023011329.py\", line 28, in objective\n",
      "    model = xgb.train(\n",
      "  File \"/home/research/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/xgboost/core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/research/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/xgboost/training.py\", line 199, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/research/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/xgboost/core.py\", line 2434, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-29 03:57:58,206] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimized_model, best_params, optimized_metrics \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_xgboost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 47\u001b[0m, in \u001b[0;36moptimize_xgboost\u001b[0;34m(X_train, y_train, X_valid, y_valid, n_trials)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Chạy Optuna\u001b[39;00m\n\u001b[1;32m     46\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest params found:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[41], line 28\u001b[0m, in \u001b[0;36moptimize_xgboost.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train with early stopping\u001b[39;00m\n\u001b[1;32m     27\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 28\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Predict và tính metric mục tiêu (WAPE)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m dvalid_pred \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_valid)\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/xgboost/core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/xgboost/training.py:199\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sales_forecasting_ai/lib/python3.10/site-packages/xgboost/core.py:2434\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2433\u001b[0m     _check_call(\n\u001b[0;32m-> 2434\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2435\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2436\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2437\u001b[0m     )\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2439\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimized_model, best_params, optimized_metrics = optimize_xgboost(\n",
    "    X_train, y_train, X_valid, y_valid, n_trials=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ed2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved submission_xgboost_optim.csv (526917 rows)\n"
     ]
    }
   ],
   "source": [
    "submission = create_xgboost_submission(df_kaggle_test, optimized_model, filename=\"submission_xgboost_optim.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcb0bd-8977-4f88-88bf-3678ec5ab50b",
   "metadata": {},
   "source": [
    "## Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306f426-a791-4612-9348-d3b4ed663e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, data):\n",
    "    \"\"\"\n",
    "    Evaluate the model performance on the test set (last 3 months of 2017)\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluating model performance on test set...\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_mae = mean_absolute_error(y_test, test_preds)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "    test_wape = weighted_absolute_percentage_error(y_test, test_preds)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"Final Model Test Evaluation:\")\n",
    "    print(f\"    MAE: {test_mae:.2f}\")\n",
    "    print(f\"    RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"    WAPE: {test_wape:.2f}%\")\n",
    "\n",
    "    # Analyze errors by time period (month)\n",
    "    test_results = data[data[\"is_test\"]].copy()\n",
    "    test_results[\"prediction\"] = test_preds\n",
    "    test_results[\"error\"] = test_results[\"sales\"] - test_results[\"prediction\"]\n",
    "    test_results[\"abs_error\"] = np.abs(test_results[\"error\"])\n",
    "    test_results[\"month_name\"] = test_results[\"date\"].dt.strftime(\"%B\")\n",
    "\n",
    "    # Summarize errors by month\n",
    "    monthly_errors = (\n",
    "        test_results.groupby(\"month_name\")\n",
    "        .agg({\"abs_error\": \"mean\", \"error\": \"mean\", \"sales\": \"mean\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    monthly_errors[\"error_pct\"] = (\n",
    "        100 * monthly_errors[\"abs_error\"] / monthly_errors[\"sales\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\nError Analysis by Month:\")\n",
    "    print(\n",
    "        monthly_errors[[\"month_name\", \"abs_error\", \"error_pct\"]].to_string(index=False)\n",
    "    )\n",
    "\n",
    "    # Store results for visualization\n",
    "    # Include month and store information for granular analysis\n",
    "    test_results[\"year_month\"] = test_results[\"date\"].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, test_preds, alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "    plt.title(\"Actual vs Predicted Sales (Test Set)\")\n",
    "    plt.xlabel(\"Actual Sales\")\n",
    "    plt.ylabel(\"Predicted Sales\")\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('actual_vs_predicted_test.png')\n",
    "\n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(test_results[\"error\"], kde=True)\n",
    "    plt.title(\"Error Distribution\")\n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('error_distribution.png')\n",
    "\n",
    "    return test_mae, test_rmse, test_wape, test_preds, y_test, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19bcf0-9d6a-4218-bba3-129fada6026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Model Results:\n",
    "# MAE: 9.03 | RMSE: 11.86 | WAPE: 29.13%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf2b3c-f81b-4746-90cc-8c4a8088a268",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lightgbm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the lightgbm model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test_mae, test_rmse, test_smape, test_preds, y_test_values, test_results \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m     evaluate_model(\u001b[43mlightgbm_model\u001b[49m, X_test, y_test, df_features)\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lightgbm_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the lightgbm model\n",
    "test_mae, test_rmse, test_smape, test_preds, y_test_values, test_results = (\n",
    "    evaluate_model(lightgbm_model, X_test, y_test, df_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e343dda-bec5-4525-91cc-a81ac25229d0",
   "metadata": {},
   "source": [
    "## Save trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8505cb-07cb-4175-9e04-5b4674befac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, X_train, feature_names, output_dir=\"../models\"):\n",
    "    \"\"\"\n",
    "    Save the trained model and related artifacts for API use\n",
    "\n",
    "    Args:\n",
    "        model: Trained model (e.g., LightGBM model)\n",
    "        feature_names: List of feature names\n",
    "        output_dir: Directory to save model artifacts\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(output_dir, \"sales_forecast_model.pkl\")\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Create and save feature statistics\n",
    "    feature_stats = {\n",
    "        \"model_version\": \"1.0.0\",\n",
    "        \"last_trained\": pd.Timestamp.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"required_columns\": list(feature_names),\n",
    "        \"column_order\": list(feature_names),\n",
    "        \"default_values\": {},\n",
    "        \"temperature_bins\": [-np.inf, 20, 25, 30, np.inf],\n",
    "        \"temperature_labels\": [\"Cold\", \"Cool\", \"Warm\", \"Hot\"],\n",
    "        \"humidity_bins\": [-np.inf, 60, 75, np.inf],\n",
    "        \"humidity_labels\": [\"Low\", \"Medium\", \"High\"],\n",
    "    }\n",
    "\n",
    "    # Add default values for date features\n",
    "    feature_stats[\"default_values\"] = {\n",
    "        \"year\": 2017,\n",
    "        \"month\": 11,\n",
    "        \"day\": 15,\n",
    "        \"day_of_week\": 2,\n",
    "        \"is_weekend\": 0,\n",
    "        \"quarter\": 4,\n",
    "        \"is_holiday\": 0,\n",
    "    }\n",
    "\n",
    "    # Save feature stats\n",
    "    stats_path = os.path.join(output_dir, \"feature_stats.json\")\n",
    "    with open(stats_path, \"w\") as f:\n",
    "        json.dump(feature_stats, f, indent=4)\n",
    "    print(f\"Feature statistics saved to {stats_path}\")\n",
    "\n",
    "    print(f\"All model artifacts saved successfully to {output_dir}/\")\n",
    "\n",
    "    return model_path, stats_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2cd1b-6d74-435c-be7c-d0ac999020d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/sales_forecast_model.pkl\n",
      "Feature statistics saved to ../models/feature_stats.json\n",
      "All model artifacts saved successfully to ../models/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/sales_forecast_model.pkl', '../models/feature_stats.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "save_model(\n",
    "    model=optimized_model,\n",
    "    X_train=X_train,\n",
    "    feature_names=X_train.columns,\n",
    "    output_dir='../models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5cc83-1c5b-4ab3-83af-cd3a6c3f00fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sales_forecasting_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
